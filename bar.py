import os
from io import BytesIO
import streamlit as st
import openai
import json
import re
import matplotlib.pyplot as plt
import numpy as np
import base64
from typing import Dict, Optional, Union, Any
from sklearn.cluster import MiniBatchKMeans
from PIL import Image

# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY = "sk-3d63de1f971640958d6cbbe98ae670b7"  # æ›¿æ¢ä¸ºä½ çš„å®é™…APIå¯†é’¥
DEEPSEEK_BASE_URL = "https://api.deepseek.com/v1"  # ç¡®è®¤æœ€æ–°APIåœ°å€

class GraphGenerator:
    def __init__(self):
        # å®šä¹‰å®‰å…¨æ‰§è¡Œç¯å¢ƒ
        self.safe_modules = {
            "plt": plt,
            "np": np,
            "math": __import__("math")
        }
        # åˆå§‹åŒ–DeepSeekå®¢æˆ·ç«¯
        self.client = openai.OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url=DEEPSEEK_BASE_URL,
        )
        # è¯»å–å½“å‰çš„è®¡æ•°å™¨å€¼
        self.data_save_folder = "generated_data_bar"  # ä¿å­˜å›¾ç‰‡çš„æ–‡ä»¶å¤¹
        if not os.path.exists(self.data_save_folder):
            os.makedirs(self.data_save_folder)  # å¦‚æœæ–‡ä»¶å¤¹ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»º

        # è¯»å–å½“å‰çš„è®¡æ•°å™¨å€¼ï¼Œè‹¥ä¸å­˜åœ¨åˆ™åˆå§‹åŒ–ä¸º 1
        self.counter_file_path = os.path.join(self.data_save_folder, "counter.txt")
        if os.path.exists(self.counter_file_path):
            with open(self.counter_file_path, "r") as f:
                self.data_counter = int(f.read())
        else:
            self.data_counter = 1  # è‹¥æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ™åˆå§‹åŒ–ä¸º 1

    def call_gpt_and_generate(
            self,
            initial_instruction: str,
            requirement: str,
            # sample_answer: str,
            student_answer: str,
            image_path: Optional[str] = None,
            model: str = "deepseek-chat",  # DeepSeekæŒ‡å®šæ¨¡å‹
            output_format: str = "json",  # æ”¯æŒjson/cod
            chartvlm_data=None) -> Union[Optional[Dict[str, str]], Any]:
        """
        ä¸»æ‰§è¡Œæ–¹æ³•ï¼šå‘é€è¯·æ±‚åˆ°DeepSeek APIå¹¶å¤„ç†å“åº”
        """
        try:
            # æ„å»ºç³»ç»ŸæŒ‡ä»¤
            system_content =  """You are an IELTS Academic Writing Task 1 expert. Your tasks:
1. Analyze student answers to extract numerical data AND trend information
2. For trend descriptions (e.g., "increased steadily", "sharp decline"):
   - Identify start/end points when explicitly mentioned
   - Estimate intermediate values using linear interpolation when only trend is described
   - Estimated values must be strings with "[est]" prefix (e.g., "[est] 85")
3. Missing data handling:
   - If a data point is completely missing and no trend is given, set the value to null.
   - If a trend is given but intermediate values are missing, you must estimate them.
4. Never include any executable code.
5. Output JSON format:
{
  "title": "...",
  "categories": [...],
  "x_label": "...",
  "y_label": "...",
  "series": [
    {
      "label": "...",
      "values": [...],
      "trend_info": [
        {"type": "increase/decrease/stable/fluctuate", "strength": "slight/moderate/dramatic", "time_range": ["start", "end"]},
        ...
      ]
    }
  ],
  "chart_type": "bar/line",
  "style": {
    "orientation": "horizontal/vertical",
    "color_palette": [],
    "estimated_values": [indexes]
  }
}

"""
            # æ„å»ºæ¶ˆæ¯é˜Ÿåˆ—
            messages = [
                {"role": "system", "content": system_content},
                {"role": "user", "content": f"Task Context: {initial_instruction}"},
                {"role": "user", "content": f"Official Requirement: {requirement}"},
                # {"role": "user", "content": f"Sample Answer: {sample_answer}"},
                {"role": "user", "content": f"""
                    ## Student Answer Analysis Task
                    * Primary Data Source: STUDENT ANSWER
                    * Conflict Resolution: Prioritize student's version when conflicting with sample
                    * Special Markers: Use [?] for unconfirmed data
                    [BEGIN STUDENT ANSWER]
                    {student_answer}
                    [END STUDENT ANSWER]
                    """}
            ]

            if image_path:
                # æå–å›¾åƒçš„é¢œè‰²è°ƒè‰²æ¿å¹¶ä½œä¸ºç”¨æˆ·æç¤º
                palette = self.extract_color_palette(image_path)
                if palette:
                    palette_str = ", ".join(f'"{c}"' for c in palette)
                    messages.append({"role": "user", "content": f"The original graph uses the following color palette: [{palette_str}]"})
                # æ·»åŠ ç¼–ç åçš„å›¾åƒ
                messages.append(self._encode_image(image_path))

            # è°ƒç”¨DeepSeek API
            response = self.client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.0,
                max_tokens=1500,
            )
            
            if not response or not hasattr(response, 'choices') or not response.choices:
                print("âš ï¸ DeepSeek APIè¿”å›ç©ºå“åº”")
                return {"error": "Empty response from DeepSeek API"}
                
            content_text = response.choices[0].message.content
            print(f"ğŸ“ DeepSeek APIè¿”å›: {content_text[:100]}...") # æ‰“å°å‰100ä¸ªå­—ç¬¦
            
            # ä¿å­˜åŸå§‹å“åº”å†…å®¹ä»¥ä¾¿è°ƒè¯•
            with open("debug.txt", "w", encoding="utf-8") as f:
                f.write(content_text)
            print("ğŸ“ å®Œæ•´å“åº”å·²ä¿å­˜åˆ° debug.txt")

            if output_format == "json":
                try:
                    # è§£æJSONå†…å®¹
                    content = self._process_response(content_text, output_format)
                    
                    # éªŒè¯å†…å®¹æ˜¯å¦åŒ…å«å¿…è¦å­—æ®µ
                    if not content.get("series") or not content.get("categories"):
                        print("âš ï¸ DeepSeekè¿”å›çš„JSONä¸åŒ…å«å¿…è¦çš„å­—æ®µ")
                        return {"error": "Invalid JSON structure - missing required fields"}
                    
                    # è¡¥å…¨è¶‹åŠ¿æ•°æ®
                    content = self._complete_trend_data(content)
                    print("ğŸ“Š deepseek JSON å¤„ç†å®Œæˆï¼ŒåŒ…å«è¶‹åŠ¿æ•°æ®")
                    
                    # ä»…å½“ chartvlm æ•°æ®å­˜åœ¨æ—¶æ‰è¿›è¡Œæ›´æ–°
                    if chartvlm_data:
                        updated_content = self.preprocess_and_update(content, chartvlm_data)
                        return updated_content
                    else:
                        return content
                except Exception as e:
                    print(f"âŒ JSONå¤„ç†é”™è¯¯: {str(e)}")
                    return {"error": f"JSON processing error: {str(e)}"}

            return {"error": "Unsupported output format"}

        except Exception as e:
            print(f"âŒ API Error: {str(e)}")
            return {"error": str(e)}

    def _encode_image(self, image_path: str) -> Dict:
        """
        å›¾ç‰‡ç¼–ç æ–¹æ³•ï¼šå°†å›¾ç‰‡è½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²å¹¶ä»¥çº¯æ–‡æœ¬å½¢å¼ä¼ é€’
        """
        # æ‰“å¼€å›¾ç‰‡å¹¶å‹ç¼©å°ºå¯¸
        with Image.open(image_path) as img:
            img.thumbnail((800, 800))  # é™åˆ¶æœ€å¤§å®½é«˜ï¼Œä¿æŒåŸå›¾æ¯”ä¾‹

            # å‹ç¼©å¹¶è½¬ä¸ºJPEGæ ¼å¼ä¿å­˜åˆ°å†…å­˜
            buffer = BytesIO()
            img.convert("RGB").save(buffer, format="JPEG", quality=70, optimize=True)

            # ç¼–ç ä¸º base64 å­—ç¬¦ä¸²
            encoded_data = base64.b64encode(buffer.getvalue()).decode("utf-8")

            # æ‹¼æ¥ä¸ºæœ€ç»ˆå†…å®¹
            content_str = f"Reference image provided: data:image/jpeg;base64,{encoded_data}"
            return {
                "role": "user",
                "content": content_str
            }

    def extract_color_palette(self, image_path: str, max_colors: int = 5) -> list:
        def rgb_to_hsv(rgb):
            # å°†RGBè½¬ä¸ºHSVè‰²å½©ç©ºé—´
            rgb = np.array(rgb) / 255.0
            maxc = np.max(rgb)
            minc = np.min(rgb)
            v = maxc
            if minc == maxc:
                return (0.0, 0.0, v)
            s = (maxc - minc) / maxc
            rc = (maxc - rgb[0]) / (maxc - minc)
            gc = (maxc - rgb[1]) / (maxc - minc)
            bc = (maxc - rgb[2]) / (maxc - minc)
            h = 0.0
            if rgb[0] == maxc:
                h = bc - gc
            elif rgb[1] == maxc:
                h = 2.0 + rc - bc
            else:
                h = 4.0 + gc - rc
            h = (h / 6.0) % 1.0
            return (h, s, v)

        with Image.open(image_path) as img:
            img = img.convert("RGB")
            if img.width * img.height > 300000:
                img = img.resize((300, 300))  # ä¿æŒæ›´å¤šç»†èŠ‚

            data = np.array(img)
            pixels = data.reshape(-1, 3)

            # å¤šç»´åº¦è¿‡æ»¤æ¡ä»¶
            filtered_pixels = []
            for pixel in pixels:
                r, g, b = [int(x) for x in pixel]
                h, s, v = rgb_to_hsv(pixel)

                cond1 = v < 0.85  # æ’é™¤è¿‡äº®é¢œè‰²
                cond2 = s > 0.25  # ä¿è¯é¥±å’Œåº¦
                cond3 = not (abs(r - g) < 25 and abs(g - b) < 25 and abs(r-b)<25)# æ’é™¤è¿‘ä¼¼ç°è‰²
                cond4 = (max(r, g, b) - min(r, g, b)) > 40

            if cond1 and cond2 and cond3 and cond4:
                    filtered_pixels.append(pixel)

            if not filtered_pixels:
                return ["#1E90FF","#FFA500" ]  # é»˜è®¤å¤‡ç”¨é¢œè‰²

            pixels = np.array(filtered_pixels, dtype=np.float32)

            # åŠ¨æ€è°ƒæ•´èšç±»æ•°é‡
            actual_colors = min(max_colors, len(np.unique(pixels, axis=0)))
            if actual_colors < 2:
                return ['#%02x%02x%02x' % tuple(pixels[0])]

            # K-meansèšç±»æ”¹è¿›ç‰ˆ

            kmeans = MiniBatchKMeans(
                n_clusters=actual_colors,
                random_state=42,
                batch_size=1024,
                n_init=3
            )
            kmeans.fit(pixels)

            # æŒ‰èšç±»å¤§å°æ’åºï¼Œä¼˜å…ˆé€‰æ‹©ä¸»è¦é¢œè‰²
            unique, counts = np.unique(kmeans.labels_, return_counts=True)
            sorted_colors = kmeans.cluster_centers_[np.argsort(-counts)]

            # è½¬æ¢ä¸ºåå…­è¿›åˆ¶
            palette = []
            for color in sorted_colors:
                hex_color = '#%02x%02x%02x' % tuple(color.astype(int))

                # ç¡®ä¿é¢œè‰²å·®å¼‚åº¦ (ä¸å·²é€‰é¢œè‰²å¯¹æ¯”)
                if not palette or all(
                        self._color_distance(hex_color, c) > 30
                        for c in palette
                ):
                    palette.append(hex_color)
                    if len(palette) >= max_colors:
                        break

            return palette

    @staticmethod
    def _color_distance(c1: str, c2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªé¢œè‰²ä¹‹é—´çš„æ¬§æ°è·ç¦»"""

        def hex_to_rgb(hex_color):
            hex_color = hex_color.lstrip('#')
            return tuple(int(hex_color[i:i + 2], 16) for i in (0, 2, 4))

        r1, g1, b1 = hex_to_rgb(c1)
        r2, g2, b2 = hex_to_rgb(c2)
        return ((r1 - r2) ** 2 + (g1 - g2) ** 2 + (b1 - b2) ** 2) ** 0.5

    def _process_response(self, content, output_format: str) -> Dict:
        """
        å“åº”å¤„ç†æ–¹æ³•
        """
        try:
            # è°ƒè¯•ï¼šå°†å®Œæ•´è¿”å›å†…å®¹å†™å…¥ debug.txt ä¾¿äºæŸ¥çœ‹
            with open("debug.txt", "w", encoding="utf-8") as f:
                f.write(content)
            print("=== Raw Response Content å·²ä¿å­˜åˆ° debug.txt ===")

            if output_format == "json":
                content = self._parse_json(content)

                # è·å–å›¾è¡¨æ–¹å‘ï¼ˆé»˜è®¤æ¨ªå‘ï¼Œå¦‚æœæ²¡æœ‰æä¾›ï¼‰
                orientation = content.get("style", {}).get("orientation", "horizontal")
                if "style" not in content:
                    content["style"] = {}
                content["style"]["orientation"] = orientation
                return content

            return self._safe_execute_code(content)
        except json.JSONDecodeError:
            return {"error": "Invalid JSON format"}
        except Exception as e:
            return {"error": str(e)}

    def _complete_trend_data(self, data: Dict) -> Dict:
        """è¡¥å…¨è¶‹åŠ¿æè¿°ä¸­çš„ç¼ºå¤±å€¼"""
        if "style" not in data:
            data["style"] = {}
        if "estimated_values" not in data["style"]:
            data["style"]["estimated_values"] = []
        
        estimated_indexes = []
        
        for series in data["series"]:
            if "trend_info" not in series:
                continue

            trend_infos = series["trend_info"]
            values = series["values"]
            
            # ç¡®ä¿ trend_info ä¸ºåˆ—è¡¨æ ¼å¼
            if isinstance(trend_infos, dict):
                trend_infos = [trend_infos]
            
            # å¤„ç†æ¯ä¸ªè¶‹åŠ¿ä¿¡æ¯
            for trend in trend_infos:
                # çº¿æ€§æ’å€¼è¡¥å…¨
                if trend["type"] in ("increase", "decrease"):
                    if "time_range" not in trend or len(trend["time_range"]) < 2:
                        continue
                        
                    try:
                        start_idx = data["categories"].index(trend["time_range"][0])
                        end_idx = data["categories"].index(trend["time_range"][1])

                        # è·å–å·²çŸ¥çš„èµ·ç‚¹å’Œç»ˆç‚¹å€¼
                        start_val = values[start_idx]
                        end_val = values[end_idx]

                        if start_val is not None and end_val is not None:
                            # çº¿æ€§æ’å€¼
                            step = (end_val - start_val) / (end_idx - start_idx)
                            for i in range(start_idx + 1, end_idx):
                                if values[i] is None:
                                    values[i] = start_val + step * (i - start_idx)
                                    estimated_indexes.append(i)
                    except (ValueError, IndexError) as e:
                        print(f"æ’å€¼é”™è¯¯: {e}, time_range: {trend['time_range']}")
                        continue

        # æ ‡è®°ä¼°ç®—å€¼
        data["style"]["estimated_values"] = estimated_indexes
        return data

    def _parse_json(self, content: str) -> Dict:
        """
        JSONè§£ææ–¹æ³•ï¼Œè‡ªåŠ¨æå– markdown æ ¼å¼ä¸­çš„ JSON éƒ¨åˆ†
        """
        # å°è¯•æå–ç¬¬ä¸€ä¸ª ```json ... ``` åŒ…è£¹çš„ JSON å†…å®¹
        json_pattern = re.compile(r"```json\s*(\{.*?\})\s*```", re.DOTALL)
        match = json_pattern.search(content)
        if match:
            json_str = match.group(1)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ° markdown åŒ…è£¹ï¼Œåˆ™å°è¯•åŒ¹é…æœ€å¤–å±‚çš„ { ... }
            json_pattern = re.compile(r"(\{.*\})", re.DOTALL)
            match = json_pattern.search(content)
            if match:
                json_str = match.group(1)
            else:
                raise ValueError("No valid JSON block found in response.")

        try:
            data = json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"âŒ JSONè§£æé”™è¯¯: {e}")
            print(f"å¯¼è‡´é”™è¯¯çš„JSONå­—ç¬¦ä¸²: {json_str[:100]}...")
            raise ValueError(f"Extracted JSON is invalid: {e}")

        # éªŒè¯JSONæ•°æ®æ ¼å¼
        required_keys = {"categories", "series"}
        if not required_keys.issubset(data.keys()):
            missing = required_keys - set(data.keys())
            print(f"âŒ JSONæ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ: {missing}")
            raise ValueError(f"Missing required keys in JSON data: {missing}")
            
        # éªŒè¯seriesæ ¼å¼
        if not isinstance(data["series"], list):
            print("âŒ 'series'å­—æ®µä¸æ˜¯åˆ—è¡¨ç±»å‹")
            raise ValueError("'series' field must be a list")
            
        # éªŒè¯categoriesæ ¼å¼
        if not isinstance(data["categories"], list):
            print("âŒ 'categories'å­—æ®µä¸æ˜¯åˆ—è¡¨ç±»å‹")
            raise ValueError("'categories' field must be a list")
            
        # ç¡®ä¿æ¯ä¸ªç³»åˆ—éƒ½æœ‰å¿…è¦çš„å­—æ®µ
        for i, series in enumerate(data["series"]):
            if not isinstance(series, dict):
                print(f"âŒ series[{i}]ä¸æ˜¯å­—å…¸ç±»å‹")
                raise ValueError(f"series[{i}] must be a dictionary")
                
            if "label" not in series:
                print(f"âŒ series[{i}]ç¼ºå°‘'label'å­—æ®µ")
                series["label"] = f"Series {i+1}"
                
            if "values" not in series:
                print(f"âŒ series[{i}]ç¼ºå°‘'values'å­—æ®µ")
                series["values"] = [None] * len(data["categories"])

        try:
            # å°è¯•ç»˜åˆ¶å›¾è¡¨
            self._plot_from_json(data)
        except Exception as e:
            print(f"âš ï¸ å›¾è¡¨ç»˜åˆ¶å¤±è´¥ï¼Œä½†ç»§ç»­å¤„ç†: {str(e)}")
            
        return data

    def parse_chartvlm_csv(self, tsv_text):
        import csv, io

        # æ›¿æ¢è½¬ä¹‰å­—ç¬¦ä¸ºçœŸå®å­—ç¬¦
        tsv_clean = tsv_text.replace("\\t", "\t").replace("\\n", "\n").strip()

        # ä½¿ç”¨ DictReader è¯»å– TSV æ•°æ®
        reader = csv.DictReader(io.StringIO(tsv_clean), delimiter="\t")
        parsed = {"categories": [], "series": []}  # æ›´æ–°ä¸ºåˆ—è¡¨ç»“æ„

        # é€è¡Œè§£ææ•°æ®
        for row in reader:
            category = None
            for header in row.keys():
                if row[header].strip():  # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæœ‰æ•°æ®çš„åˆ—ä½œä¸ºç±»åˆ«
                    category = row[header]
                    break

            if category is None:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç±»åˆ«æ•°æ®")

            # å°†ç±»åˆ«åŠ å…¥åˆ° "categories"
            parsed["categories"].append(category.strip())

            # éå†æ¯ä¸€åˆ—ï¼ˆè·³è¿‡ç±»åˆ«åˆ—ï¼‰ï¼Œå°†å…¶ä»–åˆ—çš„æ•°æ®ä½œä¸º series çš„å€¼
            for label, value in row.items():
                if label.strip().lower() in ["year", "category"]:  # è·³è¿‡ç±»åˆ«åˆ—
                    continue

                label_clean = label.strip()
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¯¥ç³»åˆ—
                series_found = False
                for series in parsed["series"]:
                    if series["label"] == label_clean:
                        series["values"].append(self.clean_value(value))
                        series_found = True
                        break

                # å¦‚æœè¯¥ç³»åˆ—è¿˜æ²¡æœ‰è®°å½•ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„ç³»åˆ—
                if not series_found:
                    parsed["series"].append({
                        "label": label_clean,
                        "values": [self.clean_value(value)]
                    })

        return parsed

    def _increment_counter(self):
        """é€’å¢è®¡æ•°å™¨å¹¶ä¿å­˜åˆ°æ–‡ä»¶ä¸­"""
        self.data_counter += 1
        with open(self.counter_file_path, "w") as f:
            f.write(str(self.data_counter))

    def preprocess_and_update(self, ds_json, chartvlm_data):
        """
        æ›¿ä»£åŸæ¥çš„ update_ds_json_with_chartvlm æ–¹æ³•ã€‚
        - å¤„ç† deepseek ä¸­åŒä¸€ label å‡ºç°å¤šæ¬¡ï¼ˆå¤šä¸ª trend_infoï¼‰çš„æƒ…å†µ
        - æŒ‰ç…§ trend_info æ£€æŸ¥æ˜¯å¦ä¸ chartvlm çš„è¶‹åŠ¿ä¸€è‡´ï¼Œè‹¥ä¸€è‡´åˆ™æ›¿æ¢ values
        """
        from copy import deepcopy
        
        # å¦‚æœ chartvlm_data ä¸ºç©ºï¼Œç›´æ¥è¿”å›åŸå§‹æ•°æ®
        if chartvlm_data is None:
            print("âš ï¸ ChartVLM æ•°æ®ä¸ºç©ºï¼Œè¿”å›åŸå§‹ deepseek æ•°æ®")
            return ds_json

        merged = {
            "title": ds_json.get("title", ""),
            "categories": ds_json.get("categories", []),
            "x_label": ds_json.get("x_label", ""),
            "y_label": ds_json.get("y_label", ""),
            "chart_type": ds_json.get("chart_type", "bar"),
            "series": [],
            "style": ds_json.get("style", {
                "orientation": "vertical",
                "color_palette": [],
                "estimated_values": []
            }),
        }

        try:
            # æ„å»º label â†’ values æ˜ å°„
            chartvlm_series_dict = {}
            for s in chartvlm_data.get("series", []):
                if "label" in s and "values" in s:
                    chartvlm_series_dict[s["label"].strip().lower()] = s["values"]
            
            print("âœ… chartvlm_series_dict keys:", list(chartvlm_series_dict.keys()))
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®ï¼Œè¿”å›åŸå§‹æ•°æ®
            if not chartvlm_series_dict:
                print("âš ï¸ ChartVLM ä¸­æ²¡æœ‰æœ‰æ•ˆçš„ series æ•°æ®ï¼Œè¿”å›åŸå§‹ deepseek æ•°æ®")
                return ds_json

            for ds_series in ds_json.get("series", []):
                print("\nğŸ” Processing DS series:", ds_series)
                if "label" not in ds_series:
                    print("âš ï¸ Series ç¼ºå°‘ labelï¼Œè·³è¿‡")
                    merged["series"].append(ds_series)
                    continue
                    
                ds_label = ds_series["label"].strip().lower()
                trend_infos = ds_series.get("trend_info", [])

                # ä¿è¯ trend_info ä¸º list
                if isinstance(trend_infos, dict):
                    trend_infos = [trend_infos]
                elif not isinstance(trend_infos, list):
                    trend_infos = []

                # æŸ¥æ‰¾å¯¹åº”çš„ chartvlm å€¼ï¼ˆå¿½ç•¥å¤§å°å†™åŒ¹é…ï¼‰
                true_values = None
                for k, v in chartvlm_series_dict.items():
                    if k == ds_label:
                        true_values = [self.clean_value(val) for val in v]
                        break

                # åŒ¹é…ä¸åˆ°å°±è·³è¿‡
                if not true_values:
                    print(f"âš ï¸ æ— æ³•åŒ¹é… label: {ds_label}ï¼Œè·³è¿‡ã€‚")
                    merged["series"].append(ds_series)
                    continue

                # æ£€æŸ¥ trend æ˜¯å¦åŒ¹é…
                matched = False
                
                # å¦‚æœæ²¡æœ‰è¶‹åŠ¿ä¿¡æ¯ï¼Œç›´æ¥ä½¿ç”¨ chartvlm å€¼
                if not trend_infos:
                    matched = True
                else:
                    for trend in trend_infos:
                        if "time_range" not in trend or "type" not in trend:
                            continue
                            
                        time_range = trend.get("time_range", [])
                        if not time_range or len(time_range) < 2:
                            continue
                            
                        trend_type = trend.get("type", "")

                        try:
                            start_idx = merged["categories"].index(time_range[0])
                            end_idx = merged["categories"].index(time_range[1]) + 1
                            
                            # ç¡®ä¿ç´¢å¼•åœ¨æœ‰æ•ˆèŒƒå›´å†…
                            if start_idx < 0 or end_idx > len(true_values) or start_idx >= end_idx:
                                continue
                                
                            print(f"ğŸ“ˆ æ£€æŸ¥è¶‹åŠ¿ï¼š{ds_series['label']} | ç´¢å¼•èŒƒå›´: {start_idx}-{end_idx - 1}")
                            sliced = true_values[start_idx:end_idx]
                            predicted_trend = self.detect_trend(sliced)
                            print(f"ğŸ” predicted: {predicted_trend}, expected: {trend_type}")
                            if predicted_trend.startswith(trend_type.replace("rapid_", "")):
                                matched = True
                                break
                        except Exception as e:
                            print(f"âš ï¸ è¶‹åŠ¿åŒ¹é…å¤±è´¥ {ds_series['label']}ï¼š{e}, time_range: {time_range}")
                            continue

                if matched:
                    new_series = deepcopy(ds_series)
                    new_series["values"] = true_values
                    merged["series"].append(new_series)
                    print(f"âœ… æ›¿æ¢ {ds_series['label']}ï¼ˆè¶‹åŠ¿åŒ¹é…ï¼‰")
                else:
                    merged["series"].append(ds_series)
                    print(f"âŒ ä¿ç•™ {ds_series['label']}ï¼ˆè¶‹åŠ¿ä¸ç¬¦æˆ–æ— åŒ¹é…ï¼‰")
                    
            return merged
            
        except Exception as e:
            print(f"âŒ preprocess_and_update é”™è¯¯: {str(e)}")
            return ds_json  # å‡ºé”™æ—¶è¿”å›åŸå§‹æ•°æ®

    def detect_trend(self, values):
        """ç®€å•è¶‹åŠ¿æ£€æµ‹ï¼ˆå‡/é™/æ³¢åŠ¨ï¼‰"""
        clean_values = [v for v in values if isinstance(v, (int, float))]
        if len(clean_values) < 2:
            return "stable"
        if all(x <= y for x, y in zip(clean_values, clean_values[1:])):
            return "increase"
        elif all(x >= y for x, y in zip(clean_values, clean_values[1:])):
            return "decrease"
        else:
            return "fluctuate"

    def clean_value(self, val):
        """æ¸…ç†å’Œè½¬æ¢å€¼ä¸ºæµ®ç‚¹æ•°ï¼Œå¤„ç†ç‰¹æ®Šæ ¼å¼"""
        if val is None:
            return None
            
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œå°è¯•æå–æ•°å€¼
        if isinstance(val, str):
            val = val.strip()
            
            # å¤„ç†ä¼°è®¡å€¼æ ‡è®°
            if val.startswith("[est]"):
                val = val[5:].strip()
                
            # å¤„ç†ç™¾åˆ†æ¯”
            if val.endswith("%"):
                val = val[:-1].strip()
                
            # æå–ç¬¬ä¸€ä¸ªæ•°å­—åºåˆ—
            match = re.search(r'[-+]?\d*\.?\d+', val)
            if match:
                try:
                    return float(match.group())
                except ValueError:
                    return None
            return None
            
        # å¦‚æœæ˜¯æ•°å€¼ï¼Œç›´æ¥è½¬æ¢
        try:
            return float(val)
        except (ValueError, TypeError):
            return None

    def _plot_from_json(self, data: Dict):
        """
        ä» JSON ç”ŸæˆæŸ±çŠ¶å›¾ï¼Œæ”¯æŒï¼š
        - æ¨ªå‘/çºµå‘æ˜¾ç¤º
        - ç¼ºå¤±å€¼é«˜äº®æ˜¾ç¤º
        - ä¼°ç®—å€¼ç‰¹æ®Šæ ‡è®°
        - è‡ªåŠ¨å¤„ç†åŒ…å«å­—ç¬¦ä¸²çš„æ•°å€¼æ•°æ®
        """
        import numpy as np

        # æ•°æ®æ¸…æ´—å‡½æ•° - å¤„ç†å¯èƒ½åŒ…å« "[est]æ•°å€¼" å­—ç¬¦ä¸²çš„æ•°æ®
        def clean_series(series):
            for entry in series:
                cleaned_values = []
                for v in entry["values"]:
                    if isinstance(v, str):
                        # æå–æ•°å­—ï¼Œæ¯”å¦‚ "[est] 81" -> 81
                        match = re.search(r'\d+', v)
                        if match:
                            cleaned_values.append(int(match.group()))
                        else:
                            cleaned_values.append(None)
                    else:
                        cleaned_values.append(v)
                entry["values"] = cleaned_values
            return series

        data["series"] = clean_series(data["series"])

        # åˆå§‹åŒ–å›¾è¡¨
        plt.figure(figsize=(12, 6))
        categories = data["categories"]
        num_categories = len(categories)
        y_pos = np.arange(num_categories)
        height = 0.35  # æŸ±çŠ¶å›¾å®½åº¦
        series = data["series"]
        colors = data.get("style", {}).get("color_palette", ["#1f77b4", "#ff7f0e", "#2ca02c"])
        orientation = data["style"].get("orientation", "horizontal")
        estimated_indexes = data.get("style", {}).get("estimated_values", [])

        # éå†æ¯ä¸ªæ•°æ®ç³»åˆ—
        for idx, series_item in enumerate(series):
            values = series_item["values"]
            label = series_item["label"]
            color = colors[idx % len(colors)]
            offset = (idx - (len(series) - 1) / 2) * height  # å¤šç³»åˆ—åç§»é‡

            # é¢„å¤„ç†æ•°æ®ï¼šåˆ†ç¦»æ­£å¸¸å€¼ã€ç¼ºå¤±å€¼å’Œä¼°ç®—å€¼
            clean_values = []
            missing_mask = []
            estimate_mask = []

            for i, val in enumerate(values):
                if val is None:
                    clean_values.append(0)  # ç¼ºå¤±å€¼è®¾ä¸º0ä»¥ä¾¿æ˜¾ç¤º
                    missing_mask.append(True)
                    estimate_mask.append(False)
                else:
                    clean_values.append(val)
                    missing_mask.append(False)
                    estimate_mask.append(i in estimated_indexes)

            # æŸ±çŠ¶å›¾ç»˜åˆ¶
            if orientation == "horizontal":
                bars = plt.barh(y_pos + offset, clean_values, height,
                                color=color, label=label, alpha=0.7)

                # æ ‡è®°ä¼°ç®—æŸ±
                for i, is_estimate in enumerate(estimate_mask):
                    if is_estimate:
                        bars[i].set_hatch('xx')
                        bars[i].set_edgecolor(color)
                        bars[i].set_alpha(0.5)
            else:
                bars = plt.bar(y_pos + offset, clean_values, height,
                               color=color, label=label, alpha=0.7)
                for i, is_estimate in enumerate(estimate_mask):
                    if is_estimate:
                        bars[i].set_hatch('xx')
                        bars[i].set_edgecolor(color)
                        bars[i].set_alpha(0.5)

            # é«˜äº®ç¼ºå¤±å€¼ï¼ˆçº¢è‰²åŠé€æ˜æ¡ï¼‰
            for i, is_missing in enumerate(missing_mask):
                if is_missing:
                    if orientation == "horizontal":
                        plt.barh(y_pos[i] + offset, 2, height,
                                 color='red', alpha=0.5, hatch='//')
                    else:
                        plt.bar(y_pos[i] + offset, 2, height,
                                color='red', alpha=0.5, hatch='//')

        # æ·»åŠ å›¾è¡¨è£…é¥°
        if orientation == "horizontal":
            plt.xlabel(data.get("y_label", "Value"))
            plt.ylabel(data.get("x_label", "Category"))
            plt.yticks(y_pos, categories)
        else:
            plt.ylabel(data.get("y_label", "Value"))
            plt.xlabel(data.get("x_label", "Category"))
            # æ ¹æ®æ ‡ç­¾é•¿åº¦è‡ªåŠ¨è°ƒæ•´æ—‹è½¬è§’åº¦
            rotation = 45 if any(len(cat) > 5 for cat in categories) else 0
            plt.xticks(y_pos, categories, rotation=rotation, ha='right' if rotation else 'center')

        # æ·»åŠ æ ‡é¢˜å’Œå›¾ä¾‹
        plt.title(data.get("title", "Student Answer Visualization"))
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

        # è‡ªåŠ¨è°ƒæ•´å¸ƒå±€
        plt.tight_layout()

        # ä¿å­˜å’Œæ˜¾ç¤ºå›¾åƒ
        data_path = os.path.join(self.data_save_folder, f"answer{self.data_counter}.png")
        plt.savefig(data_path, dpi=300, bbox_inches='tight')
        self._increment_counter()

        # Streamlit æ˜¾ç¤º
        buf = BytesIO()
        plt.savefig(buf, format='png', dpi=300, bbox_inches='tight')
        buf.seek(0)
        st.image(buf, caption=data.get("title", "Student Graph"), use_container_width=True)
        st.write(f"Data saved to: {os.path.abspath(data_path)}")

    def _safe_execute_code(self, content: str) -> Dict:
        """
        å®‰å…¨æ‰§è¡Œä»£ç æ–¹æ³•
        """
        code_block = self._extract_code(content)
        if not code_block:
            return {"error": "No valid code block"}
        try:
            restricted_globals = self.safe_modules.copy()
            local_env = {}
            exec(code_block, restricted_globals, local_env)
            plt.show()
            return {"status": "success", "output": local_env.get("figure")}
        except Exception as e:
            return {"error": f"Execution failed: {str(e)}"}

    @staticmethod
    def _extract_code(content: str) -> Optional[str]:
        """
        æå–ä»£ç å—æ–¹æ³•
        """
        match = re.search(r"```python\s*(.*?)```", content, re.DOTALL)
        return match.group(1).strip() if match else None

# ------------------ ä½¿ç”¨ç¤ºä¾‹ ------------------
if __name__ == "__main__":
    try:
        # è¾“å…¥å‚æ•°é…ç½®
        initial_instruction = (
            "Now I'll send you the Requirement, graph and Sample answer of the first Writing question of IELTS Academic. "
            "You need to learn how to reverse generate the graph according to the requirement and given answer which "
            "describes the graph by the materials I give to you."
        )

        requirement = (
            "The chart below shows the amount of leisure time enjoyed by men and women of different employment status."
            "Write a report for a university lecturer describing the information shown below."
            "Leisure time in a typical week in hour: by sex and employment status, 1998-99."
        )

        student_answer = ('''The bar chart illustrates the average number of leisure hours per week enjoyed by men and women in different employment categories in the years 1998â€“1999. The categories include full-time employment, part-time employment, unemployment, retirement, and housewives.
Overall, men tended to have more leisure time than women in most employment statuses. However, data for part-time workers and housewives is only available for females.
Among the unemployed and retired, both men and women enjoyed the most leisure time, with unemployed men averaging around 85 hours per week, slightly more than unemployed women at approximately 78 hours. A similar pattern is seen among the retired group, with men enjoying about 83 hours and women around 78 hours.
Full-time employed men had around 45 hours of leisure time, compared to about 38 hours for women in the same category. Women working part-time had approximately 40 hours of free time, while housewives had even more, averaging 50 hours per week.
In conclusion, those who were not working (either unemployed or retired) had the most leisure time, with men consistently having slightly more free time than women across similar categories.
''')

        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = GraphGenerator()
        
        # æ£€æŸ¥TSVæ•°æ®æœ‰æ•ˆæ€§
        tsv_text = """Characteristic \t Male \t Female \n Employed (Full Time) \t 44 \t 38 \n Employed (Part Time) \t 85 \t 40 \n Unemployed \t 78 \t 78 \n Retired \t 83 \t 78 \n Housewives \t 50 \t 50 \n"""
        try:
            chartvlm_data = generator.parse_chartvlm_csv(tsv_text)
            print("âœ… ChartVLMæ•°æ®è§£ææˆåŠŸ")
        except Exception as e:
            print(f"âŒ ChartVLMæ•°æ®è§£æå¤±è´¥: {str(e)}")
            chartvlm_data = None

        # æ‰§è¡ŒAPIè°ƒç”¨
        image_path = "data/bar2.png"
        # æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å­˜åœ¨
        if not os.path.exists(image_path):
            print(f"âš ï¸ è­¦å‘Š: å›¾ç‰‡ä¸å­˜åœ¨ '{image_path}'")
            
        result = generator.call_gpt_and_generate(
            initial_instruction=initial_instruction,
            requirement=requirement,
            student_answer=student_answer,
            image_path=image_path,
            output_format="json",
            chartvlm_data=chartvlm_data
        )

        # å¤„ç†ç»“æœ
        if "error" in result:
            print(f"âŒ é”™è¯¯: {result['error']}")
        else:
            print("âœ… ç”Ÿæˆæ•°æ®æˆåŠŸ:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
    
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()


